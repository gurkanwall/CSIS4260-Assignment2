{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45be448e-1ad3-472f-84c9-92eda3f44855",
   "metadata": {},
   "source": [
    "# Extracting News Articles with MechanicalSoup and BeautifulSoup\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Web scraping is a powerful technique used to extract data from websites. Two popular Python libraries for web scraping are **MechanicalSoup** and **BeautifulSoup**. Each offers distinct advantages depending on the nature of the website and the complexity of the scraping task.\n",
    "\n",
    "- **MechanicalSoup**: This library is built on top of `BeautifulSoup` and `requests`. It allows interaction with web pages, including handling forms and navigating through links, making it useful for scraping dynamic content that requires user interactions.\n",
    "\n",
    "- **BeautifulSoup**: This is a lightweight and flexible library used to parse HTML and XML documents. It provides an easy way to navigate, search, and modify the document tree, making it ideal for extracting structured data from static web pages.\n",
    "\n",
    "In this notebook, we will explore how to use **MechanicalSoup** and **BeautifulSoup** to scrape news articles from websites and compare their performance and ease of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9edaf-be62-46cf-b6a2-b7df09af5d94",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f72a13-a702-4d6c-aae0-419d356de224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed: 44 articles saved in 39.51 seconds from TechCrunch.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# TechCrunch Tech News URL\n",
    "TECHCRUNCH_URL = \"https://techcrunch.com/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Request the TechCrunch homepage\n",
    "response = requests.get(TECHCRUNCH_URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find article links\n",
    "articles = []\n",
    "for link in soup.find_all(\"a\", href=True):\n",
    "    url = link[\"href\"]\n",
    "    if url.startswith(\"https://techcrunch.com/\") and \"/20\" in url and url not in articles:\n",
    "        articles.append(url)\n",
    "\n",
    "# Scrape article details\n",
    "data = []\n",
    "for article_url in articles[:500]:  # Scraping 100 articles\n",
    "    try:\n",
    "        article_response = requests.get(article_url, headers=HEADERS)\n",
    "        article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "        title = article_soup.find(\"h1\").get_text(strip=True) if article_soup.find(\"h1\") else \"No title\"\n",
    "        content = \" \".join([p.get_text(strip=True) for p in article_soup.find_all(\"p\")])\n",
    "\n",
    "        data.append({\"title\": title, \"url\": article_url, \"content\": content})\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {article_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"techcrunch_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed: {len(data)} articles saved in {end_time - start_time:.2f} seconds from TechCrunch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9fde1-b874-4172-8bc1-4431718cca27",
   "metadata": {},
   "source": [
    "## MechanicalSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ada8e3-48be-4542-ba9c-297e4199d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed using MechanicalSoup: 44 articles saved in 8.59 seconds.\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize the browser\n",
    "browser = mechanicalsoup.StatefulBrowser(user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "# TechCrunch Tech News URL\n",
    "TECHCRUNCH_URL = \"https://techcrunch.com/\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Request the TechCrunch homepage\n",
    "browser.open(TECHCRUNCH_URL)\n",
    "\n",
    "# Find article links\n",
    "article_links = browser.links(url_regex='/20')  # Only match articles from the current year\n",
    "article_links = list(set([link.get('href') for link in article_links]))\n",
    "\n",
    "# Scrape article details\n",
    "data = []\n",
    "for article_url in article_links[:500]:  # Limit to 500 articles\n",
    "    try:\n",
    "        # Open the article page\n",
    "        browser.open(article_url)\n",
    "        \n",
    "        # Extract the article title and content\n",
    "        title = browser.page.find('h1').get_text(strip=True) if browser.page.find('h1') else 'No title'\n",
    "        content = ' '.join([p.get_text(strip=True) for p in browser.page.find_all('p')])\n",
    "        \n",
    "        # Save the data\n",
    "        data.append({\n",
    "            'title': title,\n",
    "            'url': article_url,\n",
    "            'content': content\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {article_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"techcrunch_articles_mechanicalsoup.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print completion message\n",
    "print(f\"Scraping completed using MechanicalSoup: {len(data)} articles saved in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2ce00-41b1-4110-a5df-f63d36d6baaa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this experiment, we compared the performance of **BeautifulSoup** and **MechanicalSoup** for extracting news articles from TechCrunch. The results highlight the efficiency difference between the two approaches:\n",
    "\n",
    "- **BeautifulSoup**: Scraping completed with **44 articles saved in 44.92 seconds**.\n",
    "- **MechanicalSoup**: Scraping completed with **44 articles saved in 14.35 seconds**.\n",
    "\n",
    "The significant time difference suggests that **MechanicalSoup** is much faster in this scenario, likely due to its ability to handle page interactions more efficiently. **BeautifulSoup**, while powerful for parsing and extracting structured data, relies on additional requests for fetching pages, making it slower.\n",
    "\n",
    "Overall, the choice between these libraries depends on the scraping requirements:\n",
    "- If the website requires **interacting with forms, buttons, or authentication**, **MechanicalSoup** is preferable.\n",
    "- If the website consists of **static pages with well-structured HTML**, **BeautifulSoup** provides a more flexible and intuitive approach.\n",
    "\n",
    "By selecting the appropriate tool based on the website's complexity, we can optimize web scraping tasks for both speed and accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
